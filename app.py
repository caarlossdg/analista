import streamlit as st
import requests
import os

HF_TOKEN = st.secrets.get("HF_TOKEN", os.getenv("HF_TOKEN"))
API_URL = "https://api-inference.huggingface.co/models/meta-llama/Llama-3-3.3-70B-Instruct"
HEADERS = {"Authorization": f"Bearer {HF_TOKEN}"}

def consultar_modelo(prompt):
    response = requests.post(API_URL, headers=HEADERS, json={"inputs": prompt})
    try:
        return response.json()[0]["generated_text"]
    except:
        return "丘멆잺 Error al procesar la respuesta del modelo."

st.title("游뱄 Asistente de An치lisis de Software")

apps = st.text_input("游님 Nombre(s) de la(s) aplicaci칩n(es):")
contexto = st.text_input("游꿢 쮸lg칰n contexto o uso espec칤fico?")
tipo = st.radio("游댍 Tipo de an치lisis", ["Breve", "Completo"])

if st.button("Analizar"):
    if not apps:
        st.warning("Por favor, introduce al menos una aplicaci칩n.")
    else:
        prompt = f"""
Act칰a como un asistente en castellano experto en an치lisis de software. 
Usuario ha indicado las siguientes apps: {apps}
Contexto espec칤fico: {contexto if contexto else 'Ninguno'}
Desea un an치lisis tipo: {tipo}

游대 Instrucciones generales:
- Si el usuario proporciona una sola app, analiza seg칰n la estructura detallada.
- Si da varias apps separadas por comas, comp치ralas al final.
- Adapta el an치lisis al contexto dado.
- Estructura del an치lisis:
1. Nombre y categor칤a
2. Resumen general
3. Instalaci칩n y configuraci칩n
4. Puntos fuertes
5. Puntos d칠biles
6. Impacto en la sociedad
7. Sugerencias de mejora
8. Alternativas y comparativa
9. Recomendada?
"""
        with st.spinner("Consultando modelo LLaMA en Hugging Face..."):
            salida = consultar_modelo(prompt)
            st.markdown(salida)
