import streamlit as st
import requests
import os

# ğŸ” Token Hugging Face desde secrets o variable de entorno
HF_TOKEN = st.secrets.get("HF_TOKEN", os.getenv("HF_TOKEN"))

# âœ… Modelo gratuito y estable
API_URL = "https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta"
HEADERS = {"Authorization": f"Bearer {HF_TOKEN}"}

# FunciÃ³n para consultar la API de Hugging Face
def consultar_modelo(prompt):
    try:
        response = requests.post(API_URL, headers=HEADERS, json={"inputs": prompt})
        if response.status_code != 200:
            return {"error": f"CÃ³digo de estado {response.status_code}: {response.text}"}
        result = response.json()
        return result
    except Exception as e:
        return {"error": str(e)}

# Interfaz Streamlit
st.title("ğŸ¤– Asistente de AnÃ¡lisis de Software")

apps = st.text_input("ğŸ“± Nombre(s) de la(s) aplicaciÃ³n(es):", placeholder="Ej: Replika, Notion")
contexto = st.text_input("ğŸ¯ Â¿AlgÃºn contexto o uso especÃ­fico?", placeholder="Ej: uso educativo")
tipo = st.radio("ğŸ” Tipo de anÃ¡lisis", ["Breve", "Completo"])

if st.button("Analizar"):
    if not apps:
        st.warning("Por favor, introduce al menos una aplicaciÃ³n.")
    else:
        prompt = f"""
ActÃºa como un asistente en castellano experto en anÃ¡lisis de software. 
Usuario ha indicado las siguientes apps: {apps}
Contexto especÃ­fico: {contexto if contexto else 'Ninguno'}
Desea un anÃ¡lisis tipo: {tipo}

ğŸ” Instrucciones generales:
- Si el usuario proporciona una sola app, analiza segÃºn la estructura detallada.
- Si da varias apps separadas por comas, compÃ¡ralas al final.
- Adapta el anÃ¡lisis al contexto dado.
- Estructura del anÃ¡lisis:
1. Nombre y categorÃ­a
2. Resumen general
3. InstalaciÃ³n y configuraciÃ³n
4. Puntos fuertes
5. Puntos dÃ©biles
6. Impacto en la sociedad
7. Sugerencias de mejora
8. Alternativas y comparativa
9. Â¿Recomendada?
"""

        with st.spinner("Consultando modelo en Hugging Face..."):
            resultado = consultar_modelo(prompt)
            st.write("ğŸ” Resultado crudo de la API:", resultado)  # ğŸ‘ï¸ Mostrar lo que responde la API

            # Procesar respuesta del modelo
            if "error" in resultado:
                st.error(f"âš ï¸ Error: {resultado['error']}")
            elif isinstance(resultado, list) and "generated_text" in resultado[0]:
                st.markdown(resultado[0]["generated_text"])
            elif isinstance(resultado, dict) and "generated_text" in resultado:
                st.markdown(resultado["generated_text"])
            else:
                st.error("âš ï¸ No se pudo procesar la respuesta del modelo.")
